% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/register_vae.R
\name{register_vae}
\alias{register_vae}
\alias{vae}
\title{Register Seriation Based on a Variational Autoencoder}
\usage{
register_vae()
}
\value{
Nothing.
}
\description{
Embeds the data or distance information in one dimension using a
variational autoencoder and returns the ordered points.
}
\details{
Registers the method \code{"vae"} for \code{\link[=seriate]{seriate()}}. This method learns a
1D embedding by training a variational autoencoder
(see Kingma and Welling, 2013) that
minimizes the reconstruction loss plus a penalty for
the Kullbackâ€“Leibler divergence of the
embedding space to a standard normal distribution.

The method can embed a data matrix or a distance matrix.

The chosen network topology for input data of size \eqn{n} is:

\strong{Encoder}
\itemize{
\item Input layer of size \eqn{n} accepts data \eqn{x}.
\item 1 shared dense layer of size \eqn{floor(n/2)} with relu activation.
The two output layers are directly connected to this layer.
\item first dense output layer of size 1 which produces the embedding mean
\eqn{\mu_z} with linear activation.
\item second dense output layer of size 1 for the log of the variance of \eqn{z},
\eqn{log(\sigma_z)} used to sample
with linear activation.
}

\strong{Sampler}
\itemize{
\item Samples a value \eqn{z \sim N(\mu_z, \sigma_z)}
(optional: \eqn{\sigma_z} multiplied by \code{epsilon_std}).
}

\strong{Decoder}
\itemize{
\item input layer of size 1 (takes the embedding \eqn{z}).
\item 1 dense layers of size \eqn{floor(n/2)} and relu activation.
\item dense output layer of size \eqn{n} with activation specified in \code{output_activation}
that reconstructs \eqn{x}.
The default activation function is \code{"sigmoid"} which is suitable when all data
in \eqn{x} is between 0 and 1.
Use \code{"linear"} if the data can fall outside the \eqn{[0, 1]} range.
}

\strong{Loss function}

The loss function is

\deqn{L = loss(x, \hat{x}) + \beta\ KL(N(\mu_z,\sigma_z) || N(0,1)),}

where the loss function is specified via the option \code{reconstruction_loss}.
The default for \eqn{\beta} is 1, but can be increased to create a beta-VAE.

The encoder learns \eqn{p(z|x)}, the distribution in the embedding space given
the object in the input space. The decoder learns \eqn{p(x|z)} to reconstruct
\eqn{x} given a point \eqn{z} from the embedding space.
The prior for the embedding space is chosen to be \eqn{z \sim N(1,0)}. This
distribution is enforced by the KL divergence penalty in the loss function.

Smoothness in the embedding space is created by the sampling process which
makes similar points in the embedding space decode as similar data and the
KL divergence which makes sure that points in the embedding space
are pushed together. These
properties create an embedding score whose order represents an approximate linear
order which can be used as a seriation.

\strong{Number of Epochs}
The number of epoch defaults to three times the number of observations. Early
stopping is used. This behavior can be changed via the \code{control} parameters.

\bold{Note:} Package \pkg{keras} needs to be installed.
}
\examples{
\dontrun{
register_vae()
get_seriation_method("matrix", "vae")

data("Zoo")
Zoo[,"legs"] <- (Zoo[,"legs"] > 0)
x <- as.matrix(Zoo[,-17])
label <- rownames(Zoo)
class <- Zoo$class

### embed the rows in the data matrix
o <- seriate(x, method = "vae")

pimage(x, o, prop = FALSE)

# look at the class of the animals after ordering
class[get_order(o, 1)]

# look at the embedding of the first dimension (rows)
attr(o[[1]], "vae")
attr(o[[1]], "vae")$get_embedding()
hist(attr(o[[1]], "vae")$get_embedding())

### embed the rows in the distance matrix (i.e.,
###   the distance relationship between objects)
d <- dist(x, method = "binary")

o <- seriate(d, method = "vae")
class[get_order(o)]
pimage(d, order = get_order(o))

# get the embedding
sc <- attr(o[[1]], "vae")$get_embedding()
sc

orderplot(sc, col = class)
legend("topright", legend = levels(class), col  = seq_along(levels(class)), pch = 16)
}
}
\references{
D. P. Kingma and M. Welling (2013). Auto-encoding variational bayes. ICLR.
\doi{10.48550/arXiv.1312.6114}

Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,
Xavier Glorot, Matthew Botvinick, Shakir Mohamed, Alexander Lerchner (2017).
beta-VAE: Learning Basic Visual Concepts with a Constrained
Variational Framework, International Conference on Learning Representations.
}
\seealso{
Other seriation: 
\code{\link{register_DendSer}()},
\code{\link{register_GA}()},
\code{\link{register_optics}()},
\code{\link{register_smacof}()},
\code{\link{register_tsne}()},
\code{\link{register_umap}()},
\code{\link{registry_for_seriaiton_methods}},
\code{\link{seriate_best}()},
\code{\link{seriate}()}
}
\concept{seriation}
\keyword{cluster}
\keyword{optimize}
